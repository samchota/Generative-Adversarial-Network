{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/sam/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/home/sam/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/home/sam/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/home/sam/anaconda3/lib/python3.7/imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/home/sam/anaconda3/lib/python3.7/imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.23' not found (required by /home/sam/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0m_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/imp.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(name, file, filename, details)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.23' not found (required by /home/sam/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1aa59945b3a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUpSampling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2DTranspose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZeroPadding2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolutional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Globally-importable utils.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# Try and load external backend.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 74\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/home/sam/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/home/sam/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/home/sam/anaconda3/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/home/sam/anaconda3/lib/python3.7/imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/home/sam/anaconda3/lib/python3.7/imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.23' not found (required by /home/sam/anaconda3/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import time\n",
    "import csv\n",
    "import h5py\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import BatchNormalization, Reshape, UpSampling2D, Conv2DTranspose, LeakyReLU, ZeroPadding2D, Input, MaxPooling2D, Lambda\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras import regularizers, optimizers, initializers, constraints, metrics, objectives\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.engine import InputSpec, Layer, InputLayer\n",
    "from keras.regularizers import l2\n",
    "np.random.seed(1234)\n",
    "from keras.utils import plot_model\n",
    "import keras.backend.tensorflow_backend as Kback\n",
    "from IPython.display import clear_output\n",
    "import keras as K\n",
    "import tensorflow as tf\n",
    "import scipy.io as sio\n",
    "from scipy import stats\n",
    "import scipy as scp\n",
    "from scipy.signal import resample\n",
    "import keras\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='1'\n",
    "config=tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction=1\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "def set_trainable(model, trainable):\n",
    "    model.trainable = trainable\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "#f = h5py.File('cropped_celeb.mat')\n",
    "#size= 65534\n",
    "f = h5py.File('cropped.mat')\n",
    "size = 13775"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input dimensions\n",
    "hrz = 64\n",
    "vrt = 64\n",
    "#initialize arrays\n",
    "x_train = {}\n",
    "x_train_small_pre = np.ndarray((size,hrz,vrt,3),int)\n",
    "x_train_small = np.ndarray((size,hrz,vrt,3),int)\n",
    "x_train = f['bigdata'][()]\n",
    "print(x_train.shape)\n",
    "x_train_s = np.swapaxes(x_train,0,3)\n",
    "x_train_s = np.swapaxes(x_train_s,1,2)\n",
    "print(x_train_s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,x_train_s.shape[0]):\n",
    "    pic = scp.misc.imresize(x_train_s[j,:,:,:],(hrz,vrt,3))\n",
    "    x_train_small_pre[j,:,:,:] = pic \n",
    "#x_train_small_pre = x_train_s\n",
    "print(pic[1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_small_pre.shape)\n",
    "x_train_small_pre = np.random.permutation(x_train_small_pre)\n",
    "print(x_train_small_pre.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = 1\n",
    "testpic = x_train_small_pre[probe,:,:,:]\n",
    "print(testpic.shape)\n",
    "plt.imshow(testpic.astype(np.uint8))\n",
    "print(np.amax(testpic))\n",
    "print(np.amin(testpic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#normalize images between -1 and 1\n",
    "x_train_small = (x_train_small_pre-127.5)/127.5\n",
    "\n",
    "testpic_sml = x_train_small[probe,:,:,:]\n",
    "print(\"max:\",np.amax(testpic_sml[:,:,2]))\n",
    "print(\"min:\",np.amin(testpic_sml[:,:,2]))\n",
    "\n",
    "rdiff = np.ones([hrz*vrt])\n",
    "gdiff = np.ones([hrz*vrt])\n",
    "bdiff = np.ones([hrz*vrt])\n",
    "rabsdiff= np.ones([4])\n",
    "gabsdiff= np.ones([4])\n",
    "babsdiff= np.ones([4])\n",
    "\n",
    "#get an idea of raw pixel similarities to later check for mode collapse\n",
    "for sample_in in range(4):\n",
    "    intgr = 0\n",
    "    for xax in range(hrz):\n",
    "        for yax in range(vrt):\n",
    "            rdiff[intgr] = x_train_small[0,xax,yax,0]-x_train_small[sample_in,xax,yax,0]\n",
    "            gdiff[intgr] = x_train_small[0,xax,yax,1]-x_train_small[sample_in,xax,yax,1]\n",
    "            bdiff[intgr] = x_train_small[0,xax,yax,2]-x_train_small[sample_in,xax,yax,2]\n",
    "            intgr = intgr+1\n",
    "    rabsdiff[sample_in] = 1-np.mean(abs(rdiff))\n",
    "    gabsdiff[sample_in] = 1-np.mean(abs(gdiff))\n",
    "    babsdiff[sample_in] = 1-np.mean(abs(bdiff))\n",
    "      \n",
    "print(\"similarity:\",np.mean([rabsdiff,gabsdiff,babsdiff]))\n",
    "\n",
    "testpic = ((x_train_small[probe,:,:,:]*127.5)+127.5)\n",
    "#testpic = x_train_small[probe,:,:,:]*255\n",
    "print(\"max:\",np.amax(testpic[:,:,2]))\n",
    "print(\"min:\",np.amin(testpic[:,:,2]))\n",
    "#check if we can reproduce image after normalization\n",
    "plt.imshow(testpic.astype(np.uint8)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self,param):\n",
    "        self.img_rows =hrz #image height\n",
    "        self.img_cols = vrt #image width\n",
    "        self.channels = 3 #RGB channels\n",
    "        self.img_shape = (self.img_rows,self.img_cols,self.channels)\n",
    "        self.latent_dim = 512 #dimension of latent vector\n",
    "        self.wdecay = 1e-5 #weight decay for l2 normalization (smaller weights generate simpler model)\n",
    "        self.recon_vs_gan_weight=1 #1e-4\n",
    "        learning_rate= 0.0005\n",
    "        rmsprop = RMSprop(lr = learning_rate)\n",
    "        #rmsprop = Adam(lr = learning_rate, beta_1 = 0.5, beta_2 = 0.999)\n",
    "        \n",
    "        self.encoder = self.build_encoder(int(param[0]))        \n",
    "        self.decoder = self.build_generator(int(param[0]))\n",
    "        self.discriminator = self.build_discriminator(int(param[0]))\n",
    "        \n",
    "        #Build graphs\n",
    "        #Define input shapes\n",
    "        X_im = Input(shape=(self.img_shape))\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        \n",
    "        #encoder takes a picture and spits out a latent vector\n",
    "        z_mean, z_log_var, z = self.encoder(X_im)\n",
    "        \n",
    "        #the decoder either takes a random latent vector or a latent vector generated by the encoder\n",
    "        x_gen = self.decoder(z) #encoder latent vector\n",
    "        x_p = self.decoder(noise) #random latent vector\n",
    "\n",
    "        dis_x, dis_feat_x = self.discriminator(X_im) #deciding on real images\n",
    "        dis_x_gen, dis_feat_gen = self.discriminator(x_gen) #deciding on decoded real images\n",
    "        dis_x_p  = self.discriminator(x_p)[0] #deciding on decoded fake images\n",
    "        \n",
    "        #how to punish the encoder\n",
    "        #Does the activation in the Nth layer of the discriminator look the same for a real picture\n",
    "        #and a picture that you encoded (and the generator decoded)?\n",
    "        #bincross_loss = self.enc_binary_crossentropy(dis_feat_x, dis_feat_gen)\n",
    "        layer_loss = self.MSE(dis_feat_x, dis_feat_gen)*5\n",
    "        \n",
    "        #enc_recon_loss = self.mean_gaussian_negative_log_likelihood(dis_feat_x, dis_feat_gen)\n",
    "        \n",
    "        #do the generated probability distributions of the latent space differ a lot from the normal distribution?\n",
    "        #this forces the encoder to make highly similar == compact representations\n",
    "        KL_loss = self.kl_loss(z_mean,z_log_var)\n",
    "        \n",
    "        #takes an image and spits out the activation of the discriminator at layer N (in response to any picture)\n",
    "        self.encoder_train = Model([X_im],[dis_feat_gen]) \n",
    "        self.encoder_train.add_loss(KL_loss)\n",
    "        self.encoder_train.add_loss(layer_loss)       \n",
    "       \n",
    "        #takes in an image or a random latent vector and spits out a real decoded or a fake generated image\n",
    "        #this guy also gets punished if the discriminator activates differently for real versus fake decoded images\n",
    "        \n",
    "        self.decoder_train = Model([X_im,noise], [dis_x_gen, dis_x_p])\n",
    "        #optional\n",
    "        #self.decoder_train.add_loss(layer_loss)\n",
    "                \n",
    "        #this guy takes either a real image or a (noise vector??) and spits out a decision \n",
    "        #and activations of the Nth layer for decoded images or real images.\n",
    "        #we can trace the variable [noise] back.\n",
    "        self.discriminator_train = Model([X_im, noise], [dis_x, dis_x_gen, dis_x_p])\n",
    "            \n",
    "        set_trainable(self.encoder, False)\n",
    "        set_trainable(self.decoder, False)\n",
    "        self.discriminator_train.compile(rmsprop, ['binary_crossentropy']*3)\n",
    "                          \n",
    "        set_trainable(self.discriminator, False)\n",
    "        set_trainable(self.decoder, True)\n",
    "        self.decoder_train.compile(rmsprop, ['binary_crossentropy']*2)\n",
    "        \n",
    "        #self.decoder_train.metrics_tensors.append(layer_loss)\n",
    "        #self.decoder_train.metrics_names.append(\"layer_loss\")\n",
    "                          \n",
    "        set_trainable(self.decoder, False)\n",
    "        set_trainable(self.encoder, True)\n",
    "        self.encoder_train.compile(rmsprop, ['binary_crossentropy'], loss_weights = [0])\n",
    "        \n",
    "        self.encoder_train.metrics_tensors.append(KL_loss)\n",
    "        self.encoder_train.metrics_names.append(\"KL_loss\")\n",
    "\n",
    "        self.encoder_train.metrics_tensors.append(layer_loss)\n",
    "        self.encoder_train.metrics_names.append(\"layer_loss\")\n",
    "        #self.encoder_train.compile(optimizer = rmsprop)\n",
    "\n",
    "                          \n",
    "    #Build Encoder\n",
    "    def build_encoder(self,depth):\n",
    "        dropout = 0 #0.5\n",
    "        input_shape = (hrz,vrt,3)\n",
    "        disax = -1\n",
    "        \n",
    "        X = Input(shape=input_shape)\n",
    "        #Just a bunch of convolutional layers.\n",
    "        #Batch normalization makes sure that each layers output has more or less the same distribution \n",
    "        #to not confuse the next layer over the course of the training.\n",
    "        model = Conv2D(depth*1,kernel_size =  5, strides=2,input_shape = input_shape, padding ='same', kernel_regularizer=l2(self.wdecay))(X)\n",
    "        model = BatchNormalization(axis = disax, momentum=0.8)(model)\n",
    "        model = LeakyReLU(alpha = 0.2)(model)\n",
    "        model = Dropout(dropout)(model)\n",
    "\n",
    "        model = Conv2D(depth*2,kernel_size =  5, strides=2, padding ='same', kernel_regularizer=l2(self.wdecay))(model)\n",
    "        model = BatchNormalization(axis = disax, momentum=0.8)(model)\n",
    "        model = LeakyReLU(alpha = 0.2)(model)\n",
    "        model = Dropout(dropout)(model)\n",
    "\n",
    "        model = Conv2D(depth*4,kernel_size =  5, strides=2, padding ='same', kernel_regularizer=l2(self.wdecay))(model)\n",
    "        model = BatchNormalization(axis = disax, momentum=0.8)(model)\n",
    "        model = LeakyReLU(alpha = 0.2)(model)\n",
    "        model = Dropout(dropout)(model)\n",
    "        \n",
    "        model = Conv2D(depth*4,kernel_size =  5, strides=2, padding ='same', kernel_regularizer=l2(self.wdecay))(model)\n",
    "        model = BatchNormalization(axis = disax, momentum=0.8)(model)\n",
    "        model = LeakyReLU(alpha = 0.2)(model)\n",
    "        model = Dropout(dropout)(model)\n",
    "        \n",
    "        model = Conv2D(depth*8,kernel_size =  5, strides=2, padding ='same', kernel_regularizer=l2(self.wdecay))(model)\n",
    "        model = BatchNormalization(axis = disax, momentum=0.8)(model)\n",
    "        model = LeakyReLU(alpha = 0.2)(model)\n",
    "        model = Dropout(dropout)(model)\n",
    "        \n",
    "#         model = Conv2D(depth*8,kernel_size =  5, strides=2, padding ='same',kernel_regularizer=l2(self.wdecay))(model)\n",
    "#         model = BatchNormalization(axis = disax, momentum=0.8)(model)\n",
    "#         model = LeakyReLU(alpha = 0.2)(model)\n",
    "#         model = Dropout(dropout)(model)\n",
    "        \n",
    "        model = Flatten()(model) \n",
    "        \n",
    "        #This is the variational part of the autoencoder\n",
    "        #Instead of just compressing into a meaningful Z we will learn the mean and the std of every value in Z\n",
    "        #This teaches the network that features in Z are probabilistic\n",
    "        z_mean = Dense(self.latent_dim,name ='z_mean')(model)\n",
    "        z_log_var = Dense(self.latent_dim,name ='z_var')(model)\n",
    "        #here we sample from the distributions in Z\n",
    "        z= Lambda(self.sampling, output_shape=(self.latent_dim,))([z_mean, z_log_var])\n",
    "        #the encoding model receives images and spits out Z\n",
    "        encoder_model = Model([X],[z_mean,z_log_var,z])\n",
    "        return encoder_model\n",
    "        \n",
    "    #Build generator\n",
    "    def build_generator(self,depth):\n",
    "        dropout  = 0 #0.5\n",
    "        #some stacked conv2dtranspose layers\n",
    "        generator_model = Sequential(name='generator')\n",
    "        \n",
    "        generator_model.add(InputLayer(input_shape=(self.latent_dim,)))\n",
    "        generator_model.add(Reshape((1,1,self.latent_dim)))\n",
    "        \n",
    "        generator_model.add(Conv2DTranspose(depth*8,kernel_size = 5, strides = 2, padding ='same', kernel_regularizer=l2(self.wdecay)))\n",
    "        generator_model.add(Dropout(dropout))\n",
    "        generator_model.add(BatchNormalization(momentum=0.8))\n",
    "        generator_model.add(LeakyReLU(alpha = 0.2))\n",
    "        \n",
    "#         generator_model.add(Conv2DTranspose(depth*8,kernel_size = 5, strides = 2, padding ='same', kernel_regularizer=l2(self.wdecay)))\n",
    "#         generator_model.add(Dropout(dropout))\n",
    "#         generator_model.add(BatchNormalization(momentum=0.8))\n",
    "#         generator_model.add(LeakyReLU(alpha = 0.2))\n",
    "        \n",
    "        generator_model.add(Conv2DTranspose(depth*4,kernel_size = 5, strides = 2, padding ='same', kernel_regularizer=l2(self.wdecay)))\n",
    "        generator_model.add(Dropout(dropout))\n",
    "        generator_model.add(BatchNormalization(momentum=0.8))\n",
    "        generator_model.add(LeakyReLU(alpha = 0.2))\n",
    "        \n",
    "        generator_model.add(Conv2DTranspose(depth*4,kernel_size = 5, strides = 2, padding ='same', kernel_regularizer=l2(self.wdecay)))\n",
    "        generator_model.add(Dropout(dropout))\n",
    "        generator_model.add(BatchNormalization(momentum=0.8))\n",
    "        generator_model.add(LeakyReLU(alpha = 0.2))\n",
    "        \n",
    "        generator_model.add(Conv2DTranspose(depth*2,kernel_size = 5, strides = 2, padding ='same', kernel_regularizer=l2(self.wdecay)))\n",
    "        generator_model.add(Dropout(dropout))\n",
    "        generator_model.add(BatchNormalization(momentum=0.8))\n",
    "        generator_model.add(LeakyReLU(alpha = 0.2))\n",
    "        \n",
    "        generator_model.add(Conv2DTranspose(depth*1,kernel_size = 5, strides = 2, padding ='same', kernel_regularizer=l2(self.wdecay)))\n",
    "        generator_model.add(Dropout(dropout))\n",
    "        generator_model.add(BatchNormalization(momentum=0.8))\n",
    "        generator_model.add(LeakyReLU(alpha = 0.2))\n",
    "        \n",
    "        generator_model.add(Conv2DTranspose(self.channels,kernel_size = 5, strides = 2, padding ='same', kernel_regularizer=l2(self.wdecay)))\n",
    "        generator_model.add(Activation(\"tanh\"))\n",
    "        #the generator receives a latent vector Z and spits out a 128,128,3 image\n",
    "        return generator_model\n",
    "    \n",
    "    #Build Discriminator\n",
    "    def build_discriminator(self,depth):\n",
    "        dropout = 0 #0.5\n",
    "        input_shape = (hrz,vrt,3)\n",
    "        #almost identical architecture as the encoder\n",
    "        X = Input(shape=input_shape)\n",
    "\n",
    "        modelout = Conv2D(depth*1,kernel_size =  5, strides=2,input_shape = input_shape, padding ='same', kernel_regularizer=l2(self.wdecay))(X)\n",
    "        model = BatchNormalization(momentum=0.8)(modelout)\n",
    "        model = LeakyReLU(alpha = 0.2)(model)\n",
    "        model = Dropout(dropout)(model)\n",
    "        \n",
    "        model = Conv2D(depth*2,kernel_size =  5, strides=2, padding ='same', kernel_regularizer=l2(self.wdecay))(model)\n",
    "        model = BatchNormalization(momentum=0.8)(model)\n",
    "        model = LeakyReLU(alpha = 0.2)(model)\n",
    "        model = Dropout(dropout)(model)\n",
    "\n",
    "        model = Conv2D(depth*4,kernel_size =  5, strides=2, padding ='same', kernel_regularizer=l2(self.wdecay))(model)\n",
    "        model = BatchNormalization(momentum=0.8)(model)\n",
    "        model = LeakyReLU(alpha = 0.2)(model)\n",
    "        model = Dropout(dropout)(model)\n",
    "        \n",
    "        #here we save the output of the layer as separate variable\n",
    "        model = Conv2D(depth*4,kernel_size =  5, strides=2, padding ='same', kernel_regularizer=l2(self.wdecay))(model)\n",
    "        model = BatchNormalization(momentum=0.8)(model)\n",
    "        model = LeakyReLU(alpha = 0.2)(model)\n",
    "        model = Dropout(dropout)(model)\n",
    "\n",
    "        model = Conv2D(depth*8,kernel_size =  5, strides=2, padding ='same', kernel_regularizer=l2(self.wdecay))(model)\n",
    "        \n",
    "#         model = BatchNormalization(momentum=0.8)(model)\n",
    "#         model = LeakyReLU(alpha = 0.2)(model)\n",
    "#         model = Dropout(dropout)(model)\n",
    "#         model = Conv2D(depth*8,kernel_size =  5, strides=2, padding ='same',kernel_regularizer=l2(self.wdecay))(model)\n",
    "        \n",
    "        dec = BatchNormalization(momentum=0.8)(model)\n",
    "        dec = LeakyReLU(alpha = 0.2)(dec)\n",
    "        dec = Dropout(dropout)(dec)\n",
    "        \n",
    "        dec = Flatten()(dec)\n",
    "        dec = Dense(1,activation='sigmoid', kernel_regularizer=l2(self.wdecay))(dec)\n",
    "        #discriminator takes an image as input and spits out a decision (0 or 1) and its own activation at layer N\n",
    "        output = Model([X], [dec, modelout])\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def sampling(self,args):\n",
    "        z_mean,z_log_var = args\n",
    "        epsilon = Kback.random_normal(shape = (Kback.shape(z_mean)[0],self.latent_dim), mean=0, stddev=1) \n",
    "        return z_mean + Kback.exp(z_log_var/2)*epsilon\n",
    "    \n",
    "    def enc_binary_crossentropy(self,y_true, y_pred):\n",
    "        norm = 5\n",
    "        y_true = Flatten()(y_true)\n",
    "        y_pred = Flatten()(y_pred)\n",
    "        return Kback.mean(Kback.mean(Kback.binary_crossentropy(y_true, y_pred), axis=-1))*norm\n",
    "    \n",
    "    def MSE(self,y_true, y_pred):\n",
    "        y_true = Flatten()(y_true)\n",
    "        y_pred = Flatten()(y_pred)\n",
    "        return Kback.mean(Kback.mean(Kback.square(y_pred - y_true), axis=-1))\n",
    "    \n",
    "    def kl_loss(self,z_log_var,z_mean):\n",
    "        kl_ls = Kback.mean(Kback.mean(-0.5 * Kback.sum(1 + z_log_var - Kback.square(z_mean) - Kback.exp(z_log_var), axis=-1)))\n",
    "        return kl_ls\n",
    "    \n",
    "    def mean_gaussian_negative_log_likelihood(self,y_true, y_pred):\n",
    "        y_true = Flatten()(y_true)\n",
    "        y_pred = Flatten()(y_pred)\n",
    "        nll = 0.5 * np.log(2 * np.pi) + 0.5 * Kback.square(y_pred - y_true)\n",
    "        axis = tuple(range(1, len(Kback.int_shape(y_true))))\n",
    "        return Kback.mean(Kback.sum(nll, axis=axis), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tRAINING\n",
    "epochs =3000\n",
    "batch_size = 100\n",
    "\n",
    "loopits = 1\n",
    "param1 = np.ones([2,loopits])\n",
    "fake_samples =  np.ones([loopits,4,hrz,vrt,3])\n",
    "fake_samples_training = np.ones([int(epochs/500),9,hrz,vrt,3])\n",
    "scl = 32\n",
    "plotstuff = 1\n",
    "param1[0][:] = [scl*1] \n",
    "param1[1][:] = [scl*1] \n",
    "\n",
    "samp_ind = -1\n",
    "real_index = -1\n",
    "fake_index = -1\n",
    "\n",
    "dis_loss = []\n",
    "dec_loss = []\n",
    "enc_loss = []\n",
    "\n",
    "similarity = np.ones([int(epochs/500)])\n",
    "idx = 0\n",
    "drop = -1\n",
    "for looper in range(loopits):\n",
    "    DCGAN_model = DCGAN(param1[:,looper])\n",
    "    DCGAN_model.encoder.summary()\n",
    "    DCGAN_model.decoder.summary()\n",
    "    DCGAN_model.discriminator.summary()\n",
    "    #plot_model(DCGAN_model.encoder, to_file='encoder.png')\n",
    "    #plot_model(DCGAN_model.decoder, to_file='decoder.png')\n",
    "    #plot_model(DCGAN_model.discriminator, to_file='discriminator.png')\n",
    "    y_true = np.ones([batch_size,1])\n",
    "    y_false = np.zeros([batch_size,1])\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        clear_output(wait=True)\n",
    "        #print(looper)\n",
    "        print(epoch)\n",
    "        \n",
    "        if idx+batch_size>x_train_small.shape[0]:\n",
    "            idx = 0\n",
    "        imgs = x_train_small[range(idx,idx+batch_size)]\n",
    "        idx = idx+batch_size\n",
    "        \n",
    "        noise = np.random.normal(0,1.0, (int(batch_size),DCGAN_model.latent_dim))\n",
    "        \n",
    "        #dis_x = discriminator_binary decision on real\n",
    "        #dis_x_gen = discriminator bonary decision on fake\n",
    "        #dis_feat_p = discriminator last layer output on real\n",
    "        #dis_feat_gen = discriminator last layer output on fake\n",
    "                                                                                #real,decoded real,decoded fake\n",
    "        dis_Loss = DCGAN_model.discriminator_train.train_on_batch([imgs,noise],[y_true, y_false, y_false])\n",
    "        dec_Loss = DCGAN_model.decoder_train.train_on_batch([imgs,noise],[y_true, y_true])\n",
    "        [dis_x, dis_feat_gen] = DCGAN_model.discriminator.predict(imgs)\n",
    "        enc_Loss = DCGAN_model.encoder_train.train_on_batch([imgs],[dis_feat_gen])\n",
    "\n",
    "        dis_loss.append(dis_Loss)\n",
    "        dec_loss.append(dec_Loss)\n",
    "        enc_loss.append(enc_Loss)\n",
    "        \n",
    "#         dis_loss[looper,epoch,:] = DCGAN_model.discriminator_train.train_on_batch([imgs,noise],[y_true, y_false, y_false])\n",
    "#         dec_loss[looper,epoch,:] = DCGAN_model.decoder_train.train_on_batch([imgs,noise],[y_true, y_true])\n",
    "#         [dis_x, dis_feat_gen] = DCGAN_model.discriminator.predict(imgs)\n",
    "#         enc_loss[looper,epoch,:] = np.mean(DCGAN_model.encoder_train.train_on_batch([imgs],[dis_feat_gen]))\n",
    "        \n",
    "        #discriminator: binary crossentropy                3 value prediction disc(real,decoded,fake) x100\n",
    "        #               metrics                            3\n",
    "        \n",
    "        #decoder:       binary crossentropy norm           2 value prediction disc(real,fake)         x100\n",
    "        #               metrics                            2\n",
    "        \n",
    "        #encoder:       binary crossentropy                1 value mean(4x4x128) disc activations     x100\n",
    "        #               kl_div                             1 value                                    x100\n",
    "        if epoch%500==0:\n",
    "            drop = drop+1\n",
    "            for sample in range(9):\n",
    "                noise = np.random.normal(0,1.0, size=[batch_size,DCGAN_model.latent_dim])\n",
    "                images_good_fake = DCGAN_model.decoder.predict(noise)\n",
    "                fake_sample = (np.squeeze(images_good_fake[1,:,:,:]))  \n",
    "                fake_samples_training[drop,sample,:,:,:] = fake_sample\n",
    "fake_samples_training = (fake_samples_training+1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "width = 10\n",
    "height = 10\n",
    "plt.figure(figsize=(width,height))\n",
    "fake_samples =  np.ones([loopits,9,hrz,vrt,3])\n",
    "looper = 0;\n",
    "for sample in range(9):\n",
    "    noise = np.random.normal(0,1.0, size=[batch_size,DCGAN_model.latent_dim])\n",
    "    images_good_fake = DCGAN_model.decoder.predict(noise)\n",
    "    fake_sample = (np.squeeze(images_good_fake[1,:,:,:]))  \n",
    "    fake_samples[looper,sample,:,:,:] = fake_sample\n",
    "    \n",
    "fake_samples_scaled = (fake_samples+1)/2\n",
    "plotcounter = 1    \n",
    "for looper in range(loopits):\n",
    " \n",
    "    plt.subplot(3,3,plotcounter)\n",
    "    plt.imshow(fake_samples_scaled[looper,0,:,:])\n",
    "    plt.subplot(3,3,plotcounter+1)\n",
    "    plt.imshow(fake_samples_scaled[looper,1,:,:])\n",
    "    plt.subplot(3,3,plotcounter+2)\n",
    "    plt.imshow(fake_samples_scaled[looper,2,:,:])\n",
    "    plt.subplot(3,3,plotcounter+3)\n",
    "    plt.imshow(fake_samples_scaled[looper,3,:,:])\n",
    "    plt.subplot(3,3,plotcounter+4)\n",
    "    plt.imshow(fake_samples_scaled[looper,4,:,:])\n",
    "    plt.subplot(3,3,plotcounter+5)\n",
    "    plt.imshow(fake_samples_scaled[looper,5,:,:])\n",
    "    plt.subplot(3,3,plotcounter+6)\n",
    "    plt.imshow(fake_samples_scaled[looper,6,:,:])\n",
    "    plt.subplot(3,3,plotcounter+7)\n",
    "    plt.imshow(fake_samples_scaled[looper,7,:,:])\n",
    "    plt.subplot(3,3,plotcounter+8)\n",
    "    plt.imshow(fake_samples_scaled[looper,8,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(dis_loss))\n",
    "print(np.shape(dec_loss))\n",
    "print(np.shape(enc_loss))\n",
    "\n",
    "#dis_loss_plot = [row[0:4] for row in dis_loss]\n",
    "#dec_loss_plot = ([row[0:4] for row in dec_loss]) \n",
    "#last_dec_row = [row[7] for row in dec_loss]\n",
    "enc_loss_plot = enc_loss\n",
    "dis_loss_plot = dis_loss\n",
    "dec_loss_plot = dec_loss\n",
    "\n",
    "width = 10\n",
    "height = 5\n",
    "plt.figure(figsize=(width,height))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(dis_loss_plot)\n",
    "#plt.legend(DCGAN_model.discriminator_train.metrics_names[0:3])\n",
    "plt.legend(['classification_loss','real','decoded_real','fake'])\n",
    "plt.ylim((0,10**1))\n",
    "plt.title('Discriminator')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(dec_loss_plot)\n",
    "#plt.legend(DCGAN_model.decoder_train.metrics_names[0:4])\n",
    "plt.legend(['classification_loss','recon_decoded_real','recon_fake','recon'])\n",
    "plt.ylim((0,10**2))\n",
    "plt.title('Decoder')\n",
    "\n",
    "# plt.subplot(1,4,3)\n",
    "# plt.plot(last_dec_row)\n",
    "# plt.title('Decoder_weighted_bincross')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "#plt.plot(enc_loss_plot[0,:] ,label='encoder')\n",
    "plt.plot(enc_loss_plot)\n",
    "#plt.ylim((-0,10**4))\n",
    "plt.ylim((0,2))\n",
    "plt.legend(DCGAN_model.encoder_train.metrics_names)\n",
    "plt.title('Encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "width = 10\n",
    "height = 10\n",
    "plt.figure(figsize=(width,height))\n",
    "plotcounter = 1    \n",
    "for looper in range(int(epochs/500)):\n",
    "    plt.figure(figsize=(width,height))\n",
    "    plt.subplot(3,3,plotcounter)\n",
    "    plt.imshow(fake_samples_training[looper,0,:,:])\n",
    "    plt.subplot(3,3,plotcounter+1)\n",
    "    plt.imshow(fake_samples_training[looper,1,:,:])\n",
    "    plt.subplot(3,3,plotcounter+2)\n",
    "    plt.imshow(fake_samples_training[looper,2,:,:])\n",
    "    plt.subplot(3,3,plotcounter+3)\n",
    "    plt.imshow(fake_samples_training[looper,3,:,:])\n",
    "    plt.subplot(3,3,plotcounter+4)\n",
    "    plt.imshow(fake_samples_training[looper,4,:,:])\n",
    "    plt.subplot(3,3,plotcounter+5)\n",
    "    plt.imshow(fake_samples_training[looper,5,:,:])\n",
    "    plt.subplot(3,3,plotcounter+6)\n",
    "    plt.imshow(fake_samples_training[looper,6,:,:])\n",
    "    plt.subplot(3,3,plotcounter+7)\n",
    "    plt.imshow(fake_samples_training[looper,7,:,:])\n",
    "    plt.subplot(3,3,plotcounter+8)\n",
    "    plt.imshow(fake_samples_training[looper,8,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Check encoder\n",
    "width = 10\n",
    "height = 10\n",
    "plt.figure(figsize=(width,height))\n",
    "\n",
    "plotcounter = 1\n",
    "imgs_org = x_train_small[range(1,batch_size)]\n",
    "latents = DCGAN_model.encoder.predict(imgs_org)[2]\n",
    "decoded = DCGAN_model.decoder.predict(latents)\n",
    "print(np.amax(imgs_org))\n",
    "print(np.amin(imgs_org))\n",
    "\n",
    "print(np.amax(decoded))\n",
    "print(np.amin(decoded))\n",
    "plt.subplot(2,4,plotcounter)\n",
    "plt.imshow((((imgs_org[0,:,:,:]*127.5)+127.5).astype(np.uint8)))\n",
    "plt.subplot(2,4,plotcounter+1)\n",
    "plt.imshow((((imgs_org[1,:,:,:]*127.5)+127.5).astype(np.uint8)))\n",
    "plt.subplot(2,4,plotcounter+2)\n",
    "plt.imshow((((imgs_org[2,:,:,:]*127.5)+127.5).astype(np.uint8)))\n",
    "plt.subplot(2,4,plotcounter+3)\n",
    "plt.imshow((((imgs_org[3,:,:,:]*127.5)+127.5).astype(np.uint8)))\n",
    "plt.subplot(2,4,plotcounter+4)\n",
    "plt.imshow((((decoded[0,:,:,:]*127.5)+127.5).astype(np.uint8)))\n",
    "plt.subplot(2,4,plotcounter+5)\n",
    "plt.imshow((((decoded[1,:,:,:]*127.5)+127.5).astype(np.uint8)))\n",
    "plt.subplot(2,4,plotcounter+6)\n",
    "plt.imshow((((decoded[2,:,:,:]*127.5)+127.5).astype(np.uint8)))\n",
    "plt.subplot(2,4,plotcounter+7)\n",
    "plt.imshow((((decoded[3,:,:,:]*127.5)+127.5).astype(np.uint8)))\n",
    "\n",
    "# plt.subplot(2,4,plotcounter+4)\n",
    "# plt.imshow(decoded[0,:,:,:])\n",
    "# plt.subplot(2,4,plotcounter+5)\n",
    "# plt.imshow(decoded[1,:,:,:])\n",
    "# plt.subplot(2,4,plotcounter+6)\n",
    "# plt.imshow(decoded[2,:,:,:])\n",
    "# plt.subplot(2,4,plotcounter+7)\n",
    "# plt.imshow(decoded[3,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
